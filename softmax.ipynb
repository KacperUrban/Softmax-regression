{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97c0505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76fd5a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a62674a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: \n",
      "[[1.4 0.2]\n",
      " [1.4 0.2]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]]\n",
      "Target: \n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X = iris[\"data\"][:, (2,3)]\n",
    "y = iris[\"target\"]\n",
    "print(\"Data: \")\n",
    "print(X[:5, :])\n",
    "print(\"Target: \")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4585e484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with bias: \n",
      "[[1.  1.4 0.2]\n",
      " [1.  1.4 0.2]\n",
      " [1.  1.3 0.2]\n",
      " [1.  1.5 0.2]\n",
      " [1.  1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "# Adding a bias equal to one\n",
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
    "print(\"Data with bias: \")\n",
    "print(X_with_bias[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61908547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a germ of randomness \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6ddedb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a validation, train and test set\n",
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "total_size = len(X_with_bias)\n",
    "\n",
    "test_size = int(total_size * test_ratio)\n",
    "validation_size = int(total_size * validation_ratio)\n",
    "train_size = total_size - test_size - validation_size\n",
    "\n",
    "random_indexes = np.random.permutation(total_size)\n",
    "\n",
    "X_train = X_with_bias[random_indexes[:train_size]]\n",
    "y_train = y[random_indexes[:train_size]]\n",
    "X_validation = X_with_bias[random_indexes[train_size: -validation_size]]\n",
    "y_validation = y[random_indexes[train_size: -validation_size]]\n",
    "X_test = X_with_bias[random_indexes[-test_size:]]\n",
    "y_test = y[random_indexes[-test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e039c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemenation of one hot encoding\n",
    "def to_one_hot(y):\n",
    "    number_of_classes = y.max() + 1\n",
    "    m = len(y)\n",
    "    Y_one_hot = np.zeros((m, number_of_classes))\n",
    "    Y_one_hot[np.arange(m), y] = 1\n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16d0aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_one_hot = to_one_hot(y_train)\n",
    "Y_test_one_hot = to_one_hot(y_test)\n",
    "Y_validation_one_hot = to_one_hot(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2af241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps/exp_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e34bcfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input: 3\n",
      "Number of outputs: 3\n"
     ]
    }
   ],
   "source": [
    "# Define number of input and output\n",
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y_train))\n",
    "print(\"Number of input:\", n_inputs)\n",
    "print(\"Number of outputs:\", n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "890c9b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.5356045081790177\n",
      "500 0.4711387636214726\n",
      "1000 0.36581083713176793\n",
      "1500 0.31328647878313015\n",
      "2000 0.2797297275966953\n",
      "2500 0.25570597398940204\n",
      "3000 0.23734231165431838\n",
      "3500 0.22269576109780229\n",
      "4000 0.210660238747567\n",
      "4500 0.20054860276937572\n",
      "5000 0.19190588292458005\n"
     ]
    }
   ],
   "source": [
    "# Train a softmax model\n",
    "eta = 0.05\n",
    "n_iteration = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iteration):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    if iteration%500 == 0:\n",
    "        print(iteration, loss)\n",
    "    gradients = 1/m * X_train.T.dot(error)\n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "029f75a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: \n",
      "[[ 5.60687481 -1.02439696 -7.40120651]\n",
      " [-1.2010688   0.87371521  0.43203466]\n",
      " [-2.07975966 -0.56278888  4.5593004 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Model parameters: \")\n",
    "print(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "179f4939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of class membership: \n",
      "[[2.00871059e-01 7.09394171e-01 8.97347699e-02]\n",
      " [8.94217927e-06 5.28286645e-02 9.47162393e-01]\n",
      " [1.03040300e-03 6.06212094e-01 3.92757503e-01]\n",
      " [6.87136627e-04 8.42232117e-01 1.57080746e-01]\n",
      " [4.60611763e-06 6.24005163e-02 9.37594878e-01]]\n",
      "Selecting the class to which the example belongs: \n",
      "[1 2 1 1 2]\n",
      "Accuracy_score on validation set: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "logits = X_validation.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "print(\"Calculation of class membership: \")\n",
    "print(Y_proba[:5, :])\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "print(\"Selecting the class to which the example belongs: \")\n",
    "print(y_predict[:5])\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_validation)\n",
    "print(\"Accuracy_score on validation set:\", accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64516e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.074160805836161\n",
      "500 0.5688580833116054\n",
      "1000 0.516013536950202\n",
      "1500 0.49922398558649905\n",
      "2000 0.4912573069611827\n",
      "2500 0.48686925940258124\n",
      "3000 0.484287461452685\n",
      "3500 0.48271305096298733\n",
      "4000 0.4817310402264586\n",
      "4500 0.48110891247376814\n",
      "5000 0.48071029299693224\n"
     ]
    }
   ],
   "source": [
    "# Train a softmax model with regularization\n",
    "eta = 0.05\n",
    "n_iteration = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "alpha = 0.1 # regularization hyperparameter\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iteration):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    cross_entropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2*np.sum(np.square(Theta[1:]))\n",
    "    loss = cross_entropy_loss + l2_loss * alpha\n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    if iteration%500 == 0:\n",
    "        print(iteration, loss)\n",
    "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6ed0a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameter: \n",
      "[[ 4.84714903  0.7652686  -3.7504893 ]\n",
      " [-1.07834979  0.2003082   0.87804159]\n",
      " [-0.42858153 -0.14665727  0.5752388 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Model parameter: \")\n",
    "print(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c856f32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of class membership: \n",
      "[[0.44181835 0.47111822 0.08706344]\n",
      " [0.00563123 0.26592748 0.72844129]\n",
      " [0.0257683  0.46382122 0.51041048]\n",
      " [0.01309895 0.42234828 0.56455277]\n",
      " [0.00275734 0.21715809 0.78008457]]\n",
      "Selecting the class to which the example belongs: \n",
      "[1 2 2 2 2]\n",
      "Accuracy_score on validation set: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "logits = X_validation.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "print(\"Calculation of class membership: \")\n",
    "print(Y_proba[:5, :])\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "print(\"Selecting the class to which the example belongs: \")\n",
    "print(y_predict[:5])\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_validation)\n",
    "print(\"Accuracy_score on validation set:\", accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a427e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.7755668264369415\n",
      "500 0.5936428632121784\n",
      "1000 0.5612631722586426\n",
      "1500 0.5496906967143853\n",
      "2000 0.5433929119602923\n",
      "2500 0.5395592349996164\n",
      "3000 0.5371294279788861\n",
      "3500 0.5355559777885432\n",
      "4000 0.5345206964438962\n",
      "4500 0.5338298338530085\n",
      "5000 0.5333625796433861\n",
      "5500 0.5330423817103542\n",
      "6000 0.5328200858591043\n",
      "6500 0.5326637502554392\n",
      "7000 0.5325523827470584\n",
      "7500 0.5324720364526762\n",
      "8000 0.5324133459629801\n",
      "8500 0.532369955359362\n",
      "9000 0.5323375049178377\n",
      "9500 0.5323129716458026\n",
      "10000 0.5322942361856757\n",
      "10500 0.5322797958994419\n",
      "11000 0.5322685731776343\n",
      "11500 0.5322597862875975\n",
      "12000 0.5322528616151301\n",
      "12500 0.53224737351044\n",
      "13000 0.5322430026803494\n",
      "13500 0.53223950713507\n",
      "14000 0.5322367016996726\n",
      "14500 0.5322344434158482\n",
      "15000 0.5322326210293855\n",
      "15500 0.5322311473373667\n",
      "16000 0.5322299535561513\n",
      "16500 0.5322289851316935\n",
      "17000 0.5322281985900724\n",
      "17500 0.5322275591462519\n",
      "18000 0.5322270388714817\n",
      "18500 0.5322266152766842\n",
      "19000 0.5322262702088132\n",
      "19500 0.5322259889850163\n",
      "20000 0.5322257597091614\n",
      "20500 0.5322255727294122\n",
      "21000 0.5322254202057433\n",
      "21500 0.5322252957637392\n",
      "22000 0.5322251942165338\n",
      "22500 0.5322251113408354\n",
      "23000 0.5322250436960937\n",
      "23500 0.5322249884782041\n",
      "24000 0.5322249434009585\n",
      "24500 0.5322249065998477\n",
      "25000 0.5322248765539135\n",
      "25500 0.5322248520222017\n",
      "26000 0.5322248319920537\n",
      "26500 0.5322248156369978\n",
      "27000 0.5322248022824443\n",
      "27500 0.5322247913777267\n",
      "28000 0.5322247824733035\n",
      "28500 0.5322247752021669\n",
      "29000 0.5322247692646752\n",
      "29500 0.5322247644161778\n",
      "30000 0.5322247604569166\n",
      "30500 0.5322247572237849\n",
      "31000 0.532224754583599\n",
      "31500 0.5322247524276069\n",
      "32000 0.5322247506670055\n",
      "32500 0.5322247492292798\n",
      "33000 0.5322247480552158\n",
      "33500 0.5322247470964598\n",
      "34000 0.5322247463135258\n",
      "34500 0.53222474567417\n",
      "35000 0.5322247451520619\n",
      "35500 0.5322247447256998\n",
      "36000 0.5322247443775255\n",
      "36500 0.5322247440932004\n",
      "37000 0.5322247438610153\n",
      "37500 0.5322247436714088\n",
      "38000 0.5322247435165728\n",
      "38500 0.5322247433901309\n",
      "39000 0.5322247432868762\n",
      "39500 0.5322247432025564\n",
      "40000 0.5322247431336995\n",
      "40500 0.5322247430774694\n",
      "41000 0.5322247430315509\n",
      "41500 0.532224742994053\n",
      "42000 0.5322247429634315\n",
      "42500 0.5322247429384255\n",
      "43000 0.532224742918005\n",
      "43500 0.5322247429013294\n",
      "44000 0.5322247428877118\n",
      "44500 0.5322247428765912\n",
      "45000 0.53222474286751\n",
      "45500 0.532224742860094\n",
      "46000 0.5322247428540383\n",
      "46500 0.5322247428490929\n",
      "47000 0.5322247428450544\n",
      "47500 0.5322247428417565\n",
      "48000 0.5322247428390634\n",
      "48500 0.5322247428368639\n",
      "49000 0.5322247428350679\n",
      "49500 0.5322247428336013\n",
      "50000 0.5322247428324037\n",
      "50500 0.5322247428314256\n",
      "51000 0.532224742830627\n",
      "51500 0.5322247428299747\n",
      "52000 0.5322247428294422\n",
      "52500 0.5322247428290071\n",
      "53000 0.532224742828652\n",
      "53500 0.532224742828362\n",
      "54000 0.532224742828125\n",
      "54500 0.5322247428279316\n",
      "55000 0.5322247428277737\n",
      "55500 0.5322247428276448\n",
      "55860 0.5322247428275666\n",
      "55861 0.5322247428275667 Early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train a softmax model with regularization and early stopping\n",
    "eta = 0.05\n",
    "n_iteration = 100001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "alpha = 0.1 # regularization hyperparameter\n",
    "best_loss = np.infty\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iteration):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    cross_entropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2*np.sum(np.square(Theta[1:]))\n",
    "    loss = cross_entropy_loss + l2_loss * alpha\n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "    Theta = Theta - eta * gradients\n",
    "    \n",
    "    logits = X_validation.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    cross_entropy_loss = -np.mean(np.sum(Y_validation_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2*np.sum(np.square(Theta[1:]))\n",
    "    loss = cross_entropy_loss + l2_loss * alpha\n",
    "    if iteration%500 == 0:\n",
    "        print(iteration, loss)\n",
    "    if loss <= best_loss:\n",
    "        best_loss = loss\n",
    "    else:\n",
    "        print(iteration - 1, best_loss)\n",
    "        print(iteration, loss, \"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7be70417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of class membership: \n",
      "[[0.44923503 0.47847329 0.07229168]\n",
      " [0.0052255  0.25362713 0.74114736]\n",
      " [0.02530796 0.46442003 0.51027201]\n",
      " [0.01251189 0.41276492 0.57472319]\n",
      " [0.00246856 0.20052266 0.79700878]]\n",
      "Selecting the class to which the example belongs: \n",
      "[1 2 2 2 2]\n",
      "Accuracy_score on validation set: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "logits = X_validation.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "print(\"Calculation of class membership: \")\n",
    "print(Y_proba[:5, :])\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "print(\"Selecting the class to which the example belongs: \")\n",
    "print(y_predict[:5])\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_validation)\n",
    "print(\"Accuracy_score on validation set:\", accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c305ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
