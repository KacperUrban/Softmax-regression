{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97c0505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76fd5a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a62674a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: \n",
      "[[1.4 0.2]\n",
      " [1.4 0.2]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]]\n",
      "Target: \n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X = iris[\"data\"][:, (2,3)]\n",
    "y = iris[\"target\"]\n",
    "print(\"Data: \")\n",
    "print(X[:5, :])\n",
    "print(\"Target: \")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4585e484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with bias: \n",
      "[[1.  1.4 0.2]\n",
      " [1.  1.4 0.2]\n",
      " [1.  1.3 0.2]\n",
      " [1.  1.5 0.2]\n",
      " [1.  1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "# Adding a bias equal to one\n",
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
    "print(\"Data with bias: \")\n",
    "print(X_with_bias[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61908547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a germ of randomness \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6ddedb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a validation, train and test set\n",
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "total_size = len(X_with_bias)\n",
    "\n",
    "test_size = int(total_size * test_ratio)\n",
    "validation_size = int(total_size * validation_ratio)\n",
    "train_size = total_size - test_size - validation_size\n",
    "\n",
    "random_indexes = np.random.permutation(total_size)\n",
    "\n",
    "X_train = X_with_bias[random_indexes[:train_size]]\n",
    "y_train = y[random_indexes[:train_size]]\n",
    "X_validation = X_with_bias[random_indexes[train_size: -validation_size]]\n",
    "y_validation = y[random_indexes[train_size: -validation_size]]\n",
    "X_test = X_with_bias[random_indexes[-test_size:]]\n",
    "y_test = y[random_indexes[-test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e039c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemenation of one hot encoding\n",
    "def to_one_hot(y):\n",
    "    number_of_classes = y.max() + 1\n",
    "    m = len(y)\n",
    "    Y_one_hot = np.zeros((m, number_of_classes))\n",
    "    Y_one_hot[np.arange(m), y] = 1\n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16d0aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_one_hot = to_one_hot(y_train)\n",
    "Y_test_one_hot = to_one_hot(y_test)\n",
    "Y_validation_one_hot = to_one_hot(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2af241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps/exp_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e34bcfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input: 3\n",
      "Number of outputs: 3\n"
     ]
    }
   ],
   "source": [
    "# Define number of input and output\n",
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y_train))\n",
    "print(\"Number of input:\", n_inputs)\n",
    "print(\"Number of outputs:\", n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "890c9b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.5356045081790177\n",
      "500 0.4711387636214726\n",
      "1000 0.36581083713176793\n",
      "1500 0.31328647878313015\n",
      "2000 0.2797297275966953\n",
      "2500 0.25570597398940204\n",
      "3000 0.23734231165431838\n",
      "3500 0.22269576109780229\n",
      "4000 0.210660238747567\n",
      "4500 0.20054860276937572\n",
      "5000 0.19190588292458005\n"
     ]
    }
   ],
   "source": [
    "# Train a softmax model\n",
    "eta = 0.05\n",
    "n_iteration = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iteration):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    if iteration%500 == 0:\n",
    "        print(iteration, loss)\n",
    "    gradients = 1/m * X_train.T.dot(error)\n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "029f75a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: \n",
      "[[ 5.60687481 -1.02439696 -7.40120651]\n",
      " [-1.2010688   0.87371521  0.43203466]\n",
      " [-2.07975966 -0.56278888  4.5593004 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Model parameters: \")\n",
    "print(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "179f4939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of class membership: \n",
      "[[2.00871059e-01 7.09394171e-01 8.97347699e-02]\n",
      " [8.94217927e-06 5.28286645e-02 9.47162393e-01]\n",
      " [1.03040300e-03 6.06212094e-01 3.92757503e-01]\n",
      " [6.87136627e-04 8.42232117e-01 1.57080746e-01]\n",
      " [4.60611763e-06 6.24005163e-02 9.37594878e-01]]\n",
      "Selecting the class to which the example belongs: \n",
      "[1 2 1 1 2]\n",
      "Accuracy_score on validation set: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "logits = X_validation.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "print(\"Calculation of class membership: \")\n",
    "print(Y_proba[:5, :])\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "print(\"Selecting the class to which the example belongs: \")\n",
    "print(y_predict[:5])\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_validation)\n",
    "print(\"Accuracy_score on validation set:\", accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64516e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.074160805836161\n",
      "500 0.5688580833116054\n",
      "1000 0.516013536950202\n",
      "1500 0.49922398558649905\n",
      "2000 0.4912573069611827\n",
      "2500 0.48686925940258124\n",
      "3000 0.484287461452685\n",
      "3500 0.48271305096298733\n",
      "4000 0.4817310402264586\n",
      "4500 0.48110891247376814\n",
      "5000 0.48071029299693224\n"
     ]
    }
   ],
   "source": [
    "# Train a softmax model with regularization\n",
    "eta = 0.05\n",
    "n_iteration = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "alpha = 0.1 # regularization hyperparameter\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iteration):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    cross_entropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2*np.sum(np.square(Theta[1:]))\n",
    "    loss = cross_entropy_loss + l2_loss * alpha\n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    if iteration%500 == 0:\n",
    "        print(iteration, loss)\n",
    "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6ed0a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameter: \n",
      "[[ 4.84714903  0.7652686  -3.7504893 ]\n",
      " [-1.07834979  0.2003082   0.87804159]\n",
      " [-0.42858153 -0.14665727  0.5752388 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Model parameter: \")\n",
    "print(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c856f32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of class membership: \n",
      "[[0.44181835 0.47111822 0.08706344]\n",
      " [0.00563123 0.26592748 0.72844129]\n",
      " [0.0257683  0.46382122 0.51041048]\n",
      " [0.01309895 0.42234828 0.56455277]\n",
      " [0.00275734 0.21715809 0.78008457]]\n",
      "Selecting the class to which the example belongs: \n",
      "[1 2 2 2 2]\n",
      "Accuracy_score on validation set: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "logits = X_validation.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "print(\"Calculation of class membership: \")\n",
    "print(Y_proba[:5, :])\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "print(\"Selecting the class to which the example belongs: \")\n",
    "print(y_predict[:5])\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_validation)\n",
    "print(\"Accuracy_score on validation set:\", accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a427e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.003788417612667\n",
      "500 0.6133979414623495\n",
      "1000 0.5674323513913431\n",
      "1500 0.5526881090597154\n",
      "2000 0.5450915005539835\n",
      "2500 0.5405854966707124\n",
      "3000 0.5377722833175557\n",
      "3500 0.5359689428107799\n",
      "4000 0.5347913430601116\n",
      "4500 0.5340102817271315\n",
      "5000 0.5334847703608823\n",
      "5500 0.5331263321084125\n",
      "6000 0.5328785691996291\n",
      "6500 0.5327050440922791\n",
      "7000 0.5325819247668686\n",
      "7500 0.5324934434460067\n",
      "8000 0.5324290518698223\n",
      "8500 0.5323816167148033\n",
      "9000 0.5323462617896006\n",
      "9500 0.5323196174637647\n",
      "10000 0.5322993293998309\n",
      "10500 0.532283734084204\n",
      "11000 0.5322716426545907\n",
      "11500 0.5322621956466723\n",
      "12000 0.5322547645583572\n",
      "12500 0.5322488845595993\n",
      "13000 0.5322442080784449\n",
      "13500 0.5322404724857557\n",
      "14000 0.5322374773749681\n",
      "14500 0.5322350684243125\n",
      "15000 0.5322331258129099\n",
      "15500 0.5322315558154563\n",
      "16000 0.5322302846364955\n",
      "16500 0.5322292538383331\n",
      "17000 0.5322284169146879\n",
      "17500 0.5322277366968162\n",
      "18000 0.5322271833710248\n",
      "18500 0.532226732950002\n",
      "19000 0.5322263660845615\n",
      "19500 0.5322260671332778\n",
      "20000 0.532225823429371\n",
      "20500 0.5322256246997705\n",
      "21000 0.5322254626025326\n",
      "21500 0.5322253303569614\n",
      "22000 0.5322252224468106\n",
      "22500 0.5322251343814084\n",
      "23000 0.5322250625029229\n",
      "23500 0.532225003830523\n",
      "24000 0.5322249559341555\n",
      "24500 0.5322249168321619\n",
      "25000 0.5322248849081265\n",
      "25500 0.5322248588432844\n",
      "26000 0.5322248375615283\n",
      "26500 0.5322248201846353\n",
      "27000 0.5322248059957975\n",
      "27500 0.5322247944098986\n",
      "28000 0.532224784949284\n",
      "28500 0.5322247772240005\n",
      "29000 0.5322247709156769\n",
      "29500 0.5322247657643728\n",
      "30000 0.532224761557849\n",
      "30500 0.5322247581228076\n",
      "31000 0.5322247553177447\n",
      "31500 0.5322247530271156\n",
      "32000 0.5322247511565698\n",
      "32500 0.5322247496290634\n",
      "33000 0.5322247483816843\n",
      "33500 0.5322247473630581\n",
      "34000 0.5322247465312341\n",
      "34500 0.5322247458519539\n",
      "35000 0.5322247452972432\n",
      "35500 0.5322247448442572\n",
      "36000 0.5322247444743414\n",
      "36500 0.532224744172262\n",
      "37000 0.5322247439255785\n",
      "37500 0.5322247437241323\n",
      "38000 0.5322247435596277\n",
      "38500 0.5322247434252904\n",
      "39000 0.5322247433155879\n",
      "39500 0.532224743226003\n",
      "40000 0.5322247431528462\n",
      "40500 0.5322247430931051\n",
      "41000 0.5322247430443192\n",
      "41500 0.5322247430044799\n",
      "42000 0.5322247429719464\n",
      "42500 0.5322247429453789\n",
      "43000 0.5322247429236833\n",
      "43500 0.5322247429059663\n",
      "44000 0.5322247428914982\n",
      "44500 0.5322247428796835\n",
      "45000 0.5322247428700353\n",
      "45500 0.5322247428621563\n",
      "46000 0.5322247428557221\n",
      "46500 0.532224742850468\n",
      "47000 0.5322247428461774\n",
      "47500 0.5322247428426735\n",
      "48000 0.5322247428398121\n",
      "48500 0.5322247428374756\n",
      "49000 0.5322247428355675\n",
      "49500 0.5322247428340092\n",
      "50000 0.5322247428327368\n",
      "50500 0.5322247428316977\n",
      "51000 0.5322247428308491\n",
      "51500 0.5322247428301561\n",
      "52000 0.5322247428295903\n",
      "52500 0.5322247428291282\n",
      "53000 0.5322247428287508\n",
      "53500 0.5322247428284426\n",
      "54000 0.532224742828191\n",
      "54500 0.5322247428279854\n",
      "55000 0.5322247428278176\n",
      "55500 0.5322247428276806\n",
      "56000 0.5322247428275688\n",
      "56320 0.532224742827508\n",
      "56321 0.5322247428275081 Early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train a softmax model with regularization and early stopping\n",
    "eta = 0.05\n",
    "n_iteration = 100001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "alpha = 0.1 # regularization hyperparameter\n",
    "best_loss = np.infty\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iteration):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    cross_entropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2*np.sum(np.square(Theta[1:]))\n",
    "    loss = cross_entropy_loss + l2_loss * alpha\n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "    Theta = Theta - eta * gradients\n",
    "    \n",
    "    logits = X_validation.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    cross_entropy_loss = -np.mean(np.sum(Y_validation_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2*np.sum(np.square(Theta[1:]))\n",
    "    loss = cross_entropy_loss + l2_loss * alpha\n",
    "    if iteration%500 == 0:\n",
    "        print(iteration, loss)\n",
    "    if loss <= best_loss:\n",
    "        best_loss = loss\n",
    "    else:\n",
    "        print(iteration - 1, best_loss)\n",
    "        print(iteration, loss, \"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac0d0ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of class membership: \n",
      "[[0.44923503 0.47847329 0.07229168]\n",
      " [0.0052255  0.25362713 0.74114736]\n",
      " [0.02530796 0.46442003 0.51027201]\n",
      " [0.01251189 0.41276492 0.57472319]\n",
      " [0.00246856 0.20052266 0.79700878]]\n",
      "Selecting the class to which the example belongs: \n",
      "[1 2 2 2 2]\n",
      "Accuracy_score on validation set: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "logits = X_validation.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "print(\"Calculation of class membership: \")\n",
    "print(Y_proba[:5, :])\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "print(\"Selecting the class to which the example belongs: \")\n",
    "print(y_predict[:5])\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_validation)\n",
    "print(\"Accuracy_score on validation set:\", accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32a2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
